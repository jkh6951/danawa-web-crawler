from fastapi import FastAPI, Request, WebSocket, WebSocketDisconnect, Form, BackgroundTasks
from fastapi.templating import Jinja2Templates
from fastapi.responses import HTMLResponse, FileResponse
import json
import uuid
from datetime import datetime
import requests
from bs4 import BeautifulSoup
import csv
import time
import random
import re
from urllib.parse import quote
from typing import List

# FastAPI ì•± ìƒì„±
app = FastAPI(title="ë‹¤ë‚˜ì™€ í”„ë¡œ í¬ë¡¤ëŸ¬")

# í…œí”Œë¦¿ ì„¤ì •
templates = Jinja2Templates(directory="templates")

# í¬ë¡¤ë§ ì‘ì—… ìƒíƒœ ì €ì¥
crawling_jobs = {}
active_connections: List[WebSocket] = []

class DanawaWebCrawler:
    def __init__(self, job_id: str):
        self.job_id = job_id
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        self.status = "ì¤€ë¹„ì¤‘"
        self.progress = 0
        self.total_items = 0
        self.current_item = 0
        self.results = []
        
    async def notify_progress(self, message: str):
        """WebSocketìœ¼ë¡œ ì§„í–‰ìƒí™© ì „ì†¡"""
        for connection in active_connections:
            try:
                await connection.send_text(json.dumps({
                    "job_id": self.job_id,
                    "status": self.status,
                    "progress": self.progress,
                    "current_item": self.current_item,
                    "total_items": self.total_items,
                    "message": message
                }))
            except:
                pass
    
    async def crawl_danawa(self, keyword: str, max_pages: int = 3):
        """ë‹¤ë‚˜ì™€ í¬ë¡¤ë§ ë©”ì¸ í•¨ìˆ˜"""
        try:
            self.status = "ì‹œì‘"
            await self.notify_progress(f"'{keyword}' ê²€ìƒ‰ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
            
            # ê¸°ë³¸ ì •ë³´ ìˆ˜ì§‘
            basic_products = await self.collect_basic_info(keyword, max_pages)
            
            if not basic_products:
                self.status = "ì‹¤íŒ¨"
                await self.notify_progress("ìƒí’ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return []
            
            self.total_items = len(basic_products)
            self.status = "ì™„ë£Œ"
            self.results = basic_products
            await self.notify_progress(f"ì´ {len(basic_products)}ê°œ ìƒí’ˆ ìˆ˜ì§‘ ì™„ë£Œ!")
            
            return basic_products
            
        except Exception as e:
            self.status = "ì˜¤ë¥˜"
            await self.notify_progress(f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return []
    
    async def collect_basic_info(self, keyword: str, max_pages: int):
        """ê¸°ë³¸ ìƒí’ˆ ì •ë³´ ìˆ˜ì§‘"""
        products = []
        
        for page in range(1, max_pages + 1):
            await self.notify_progress(f"{page}í˜ì´ì§€ ìˆ˜ì§‘ ì¤‘...")
            
            url = f"https://search.danawa.com/dsearch.php?query={keyword}&sort=opinionDESC&list=list&boost=true&limit=40&mode=simple&page={page}"
            
            try:
                response = self.session.get(url, timeout=15)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.text, 'html.parser')
                page_products = self.extract_products_from_page(soup)
                
                if not page_products:
                    await self.notify_progress(f"{page}í˜ì´ì§€ì—ì„œ ìƒí’ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    break
                
                products.extend(page_products)
                await self.notify_progress(f"{page}í˜ì´ì§€: {len(page_products)}ê°œ ìƒí’ˆ ë°œê²¬")
                
                time.sleep(random.uniform(1, 2))
                
            except Exception as e:
                await self.notify_progress(f"{page}í˜ì´ì§€ ì˜¤ë¥˜: {str(e)}")
                break
        
        return products
    
    def extract_products_from_page(self, soup):
        """í˜ì´ì§€ì—ì„œ ìƒí’ˆ ì •ë³´ ì¶”ì¶œ"""
        products = []
        selectors = ['ul.product_list li', '.main_prodlist li', '.prod_list li', 'li.prod_item']
        
        items = []
        for selector in selectors:
            items = soup.select(selector)
            if items:
                break
        
        for item in items:
            try:
                product = self.extract_single_product(item)
                if product:
                    products.append(product)
            except:
                continue
                
        return products
    
    def extract_single_product(self, item):
        """ê°œë³„ ìƒí’ˆ ì •ë³´ ì¶”ì¶œ"""
        name = self.get_product_name(item)
        if not name or len(name.strip()) < 3:
            return None
        
        price = self.get_price(item)
        product_url = self.get_product_url(item)
        
        if name and price > 0:
            return {
                'name': name.strip(),
                'price': price,
                'product_url': product_url,
                'coupang_search_url': f"https://www.coupang.com/np/search?q={quote(name.strip())}"
            }
        
        return None
    
    def get_product_name(self, item):
        """ìƒí’ˆëª… ì¶”ì¶œ"""
        selectors = ['p.prod_name a', 'dt.prod_name a', 'div.prod_name a', 'a.prod_name', '.prod_name a']
        
        for selector in selectors:
            elem = item.select_one(selector)
            if elem:
                text = elem.get_text(strip=True)
                if text and len(text) > 3:
                    return self.clean_name(text)
                
                title = elem.get('title', '').strip()
                if title and len(title) > 3:
                    return self.clean_name(title)
        
        return None
    
    def get_price(self, item):
        """ê°€ê²© ì¶”ì¶œ"""
        selectors = ['strong.num', 'em.num_c', '.price strong', 'span.price', '.price_sect strong']
        
        for selector in selectors:
            elem = item.select_one(selector)
            if elem:
                price_text = re.sub(r'[^\d]', '', elem.get_text())
                if price_text:
                    try:
                        return int(price_text)
                    except:
                        continue
        
        return 0
    
    def get_product_url(self, item):
        """ìƒí’ˆ URL ì¶”ì¶œ"""
        selectors = ['p.prod_name a', 'dt.prod_name a', 'a.prod_name']
        
        for selector in selectors:
            elem = item.select_one(selector)
            if elem:
                href = elem.get('href', '')
                if href:
                    if href.startswith('//'):
                        return 'https:' + href
                    elif href.startswith('/'):
                        return 'https://www.danawa.com' + href
                    return href
        
        return ''
    
    def clean_name(self, name):
        """ìƒí’ˆëª… ì •ë¦¬"""
        import html
        name = html.unescape(name)
        
        patterns = [r'\[.*?ë¬´ë£Œë°°ì†¡.*?\]', r'\[.*?íŠ¹ê°€.*?\]', r'\[.*?ì´ë²¤íŠ¸.*?\]', r'â˜…+', r'â˜†+']
        
        for pattern in patterns:
            name = re.sub(pattern, '', name, flags=re.IGNORECASE)
        
        return re.sub(r'\s+', ' ', name).strip()

# API ì—”ë“œí¬ì¸íŠ¸ë“¤
@app.get("/")
async def read_root():
    """ë©”ì¸ í˜ì´ì§€"""
    return {
        "message": "ğŸ›’ ë‹¤ë‚˜ì™€ í¬ë¡¤ëŸ¬ê°€ ì‘ë™ ì¤‘ì…ë‹ˆë‹¤!", 
        "status": "OK",
        "endpoints": {
            "í¬ë¡¤ë§ ì‹œì‘": "/api/crawl/start",
            "í…ŒìŠ¤íŠ¸": "/test",
            "API ë¬¸ì„œ": "/docs"
        }
    }

@app.get("/test")
async def test():
    return {"message": "ì„œë²„ê°€ ì‚´ì•„ìˆì–´ìš”!", "status": "OK"}

@app.post("/api/crawl/start")
async def start_crawling(background_tasks: BackgroundTasks, keyword: str = Form(...), pages: int = Form(...)):
    """í¬ë¡¤ë§ ì‹œì‘"""
    job_id = str(uuid.uuid4())
    
    crawler = DanawaWebCrawler(job_id)
    crawling_jobs[job_id] = crawler
    
    background_tasks.add_task(crawler.crawl_danawa, keyword, pages)
    
    return {"job_id": job_id, "status": "ì‹œì‘ë¨", "message": f"'{keyword}' í¬ë¡¤ë§ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤."}

@app.get("/api/crawl/status/{job_id}")
async def get_crawling_status(job_id: str):
    """í¬ë¡¤ë§ ìƒíƒœ ì¡°íšŒ"""
    if job_id not in crawling_jobs:
        return {"error": "ì‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
    
    crawler = crawling_jobs[job_id]
    return {
        "job_id": job_id,
        "status": crawler.status,
        "progress": crawler.progress,
        "current_item": crawler.current_item,
        "total_items": crawler.total_items,
        "result_count": len(crawler.results)
    }

@app.get("/api/crawl/results/{job_id}")
async def get_crawling_results(job_id: str):
    """í¬ë¡¤ë§ ê²°ê³¼ ì¡°íšŒ"""
    if job_id not in crawling_jobs:
        return {"error": "ì‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
    
    crawler = crawling_jobs[job_id]
    return {
        "job_id": job_id,
        "status": crawler.status,
        "results": crawler.results,
        "total_count": len(crawler.results)
    }

@app.get("/api/crawl/download/{job_id}")
async def download_results(job_id: str, format: str = "csv"):
    """ê²°ê³¼ ë‹¤ìš´ë¡œë“œ"""
    if job_id not in crawling_jobs:
        return {"error": "ì‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
    
    crawler = crawling_jobs[job_id]
    
    if format == "csv":
        filename = f"danawa_results_{job_id[:8]}.csv"
        
        with open(filename, 'w', newline='', encoding='utf-8-sig') as f:
            if crawler.results:
                fieldnames = ['name', 'price', 'product_url', 'coupang_search_url']
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                writer.writeheader()
                writer.writerows(crawler.results)
        
        return FileResponse(filename, filename=filename)
    
    elif format == "json":
        filename = f"danawa_results_{job_id[:8]}.json"
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(crawler.results, f, ensure_ascii=False, indent=2)
        
        return FileResponse(filename, filename=filename)

@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    """WebSocket ì—°ê²°"""
    await websocket.accept()
    active_connections.append(websocket)
    
    try:
        while True:
            await websocket.receive_text()
    except WebSocketDisconnect:
        active_connections.remove(websocket)
